/**
 * predictions.js
 * Manages webcam, audio, and model predictions
 * Supports image, pose, and audio models from Teachable Machine
 */

import { getModel, getModelType } from './model-loader.js';
import { sendToMicrobit, isConnected } from './bluetooth.js';

// State
let webcam = null;
let predictionCanvas = null; // offscreen 200x200 canvas for pose prediction
let isRunning = false;
let currentFacingMode = 'user'; // 'user' = front camera, 'environment' = back camera
let audioContext = null;
let analyser = null;
let microphone = null;

/**
 * Replace the TM Webcam's internal stream with an environment-facing stream.
 * Called after webcam.setup() which always opens the front camera.
 *
 * Key points:
 *  - No 'exact' facingMode constraint ‚Üí maximum Android/iOS compatibility
 *  - Short delay after stopping tracks ‚Üí Android needs ~200 ms to release the
 *    camera hardware before another stream can be opened
 *  - No play() call here ‚Üí webcam.play() is the sole caller; a double play()
 *    causes AbortError on Chrome Android and triggers the error toast
 *  - Object.values() to find the HTMLVideoElement ‚Üí works regardless of the
 *    internal property name used by different TM library builds
 */
async function applyEnvironmentCamera(webcamInstance) {
    // Locate the internal <video> element regardless of TM version property name.
    // TM image/pose v0.8.x stores it as 'webcam'; other builds may use 'video'.
    const videoEl = Object.values(webcamInstance).find(v => v instanceof HTMLVideoElement);

    // Stop the active stream ‚Äî the real stream lives on the video element's srcObject,
    // not on a separate 'stream' property (TM v0.8.x has no such property).
    const activeStream = (videoEl && videoEl.srcObject) || webcamInstance.stream;
    if (activeStream) {
        activeStream.getTracks().forEach(t => t.stop());
    }
    if (webcamInstance.stream) webcamInstance.stream = null;
    if (videoEl) videoEl.srcObject = null;

    // Wait for the camera hardware to fully release (important on Android)
    await new Promise(r => setTimeout(r, 200));

    // Request the back camera ‚Äî no 'exact' so the browser can fall back gracefully
    const stream = await navigator.mediaDevices.getUserMedia({
        audio: false,
        video: {
            facingMode: 'environment',
            width: { ideal: webcamInstance.width || 400 },
            height: { ideal: webcamInstance.height || 400 }
        }
    });

    if (webcamInstance.stream !== undefined) webcamInstance.stream = stream;
    if (videoEl) {
        videoEl.srcObject = stream;
        // Do NOT call play() here ‚Äî webcam.play() is called next and must be
        // the only caller; duplicate play() causes AbortError on Chrome Android
    }
}

/**
 * Start predictions based on model type
 */
async function startPredictions() {
    const modelType = getModelType();
    isRunning = true;
    
    console.log(`‚ñ∂Ô∏è Starting predictions for model type: ${modelType}`);
    
    if (modelType === 'image') {
        await setupWebcam();
    } else if (modelType === 'pose') {
        await setupPoseWebcam();
    } else if (modelType === 'audio') {
        await setupAudio();
    }
}

/**
 * Stop all predictions and cleanup resources
 */
function stopPredictions() {
    isRunning = false;
    
    // Cleanup webcam
    if (webcam) {
        webcam.stop();
        webcam = null;
    }
    predictionCanvas = null;
    
    // Cleanup audio
    if (microphone) {
        microphone.getTracks().forEach(track => track.stop());
        microphone = null;
    }
    
    if (audioContext) {
        audioContext.close();
        audioContext = null;
    }
    
    // Stop audio model
    const model = getModel();
    const modelType = getModelType();
    if (model && modelType === 'audio' && model.stopListening) {
        model.stopListening();
    }
}

/**
 * Setup webcam for image classification models
 */
async function setupWebcam() {
    try {
        const flip = currentFacingMode === 'user';
        webcam = new window.tmImage.Webcam(400, 400, flip);

        await webcam.setup();
        if (currentFacingMode === 'environment') {
            await applyEnvironmentCamera(webcam);
        }
        await webcam.play();

        const wrapper = document.getElementById('webcam-wrapper');
        wrapper.innerHTML = '';
        wrapper.appendChild(webcam.canvas);
        window.requestAnimationFrame(loopImage);
    } catch (error) {
        console.error('Webcam error:', error);
        throw error;
    }
}

/**
 * Setup webcam for pose detection models
 * webcam.canvas is kept off-DOM (capture only).
 * A separate pose-display canvas is shown to the user so its context is
 * always clean ‚Äî no transform side-effects from webcam.update().
 * An offscreen 200x200 predictionCanvas is used for estimatePose so
 * keypoint coordinates stay in the 0-200 range the TM classifier expects.
 */
async function setupPoseWebcam() {
    try {
        console.log('ü¶¥ Setting up pose webcam...');

        // 400x400 webcam ‚Äî NOT added to the DOM, used only for video capture
        const flip = currentFacingMode === 'user';
        webcam = new window.tmPose.Webcam(400, 400, flip);

        await webcam.setup();
        if (currentFacingMode === 'environment') {
            await applyEnvironmentCamera(webcam);
        }
        await webcam.play();

        // 200x200 offscreen canvas for pose estimation
        predictionCanvas = document.createElement('canvas');
        predictionCanvas.width = 200;
        predictionCanvas.height = 200;

        // 400x400 canvas shown to the user
        const displayCanvas = document.createElement('canvas');
        displayCanvas.id = 'pose-display';
        displayCanvas.width = 400;
        displayCanvas.height = 400;

        const wrapper = document.getElementById('webcam-wrapper');
        wrapper.innerHTML = '';
        wrapper.appendChild(displayCanvas);

        console.log('ü¶¥ Pose webcam ready, starting prediction loop...');
        window.requestAnimationFrame(loopPose);
    } catch (error) {
        console.error('Pose webcam error:', error);
        throw error;
    }
}

/**
 * Setup audio for voice/sound classification models
 */
async function setupAudio() {
    try {
        console.log('üé§ Setting up audio...');
        
        // Create visualization canvas
        const canvas = document.createElement('canvas');
        canvas.width = 400;
        canvas.height = 400;
        canvas.id = 'audio-visualizer';
        
        const wrapper = document.getElementById('webcam-wrapper');
        wrapper.innerHTML = '';
        wrapper.appendChild(canvas);
        
        // Setup audio context for visualization
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        microphone = stream;
        
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        analyser = audioContext.createAnalyser();
        analyser.fftSize = 2048;
        
        const source = audioContext.createMediaStreamSource(stream);
        source.connect(analyser);
        
        // Start visualization
        window.requestAnimationFrame(visualizeAudio);
        
        // Start audio model listening
        const model = getModel();
        console.log('üé§ Starting audio recognition...');
        
        await model.listen(result => {
            const wordLabels = model.wordLabels();
            const scoresArray = Array.from(result.scores);
            
            const predictions = scoresArray.map((score, i) => ({
                className: wordLabels[i] || `Class ${i}`,
                probability: score
            }));
            
            displayPredictions(predictions);
        }, {
            includeSpectrogram: false,
            probabilityThreshold: 0.5,
            invokeCallbackOnNoiseAndUnknown: true,
            overlapFactor: 0.5
        });
        
        console.log('‚úÖ Audio configured');
    } catch (error) {
        console.error('Audio setup error:', error);
        throw error;
    }
}

/**
 * Visualize audio as frequency bars
 * Optimized for human voice (80Hz - 8000Hz)
 */
function visualizeAudio() {
    if (!isRunning || !analyser) return;
    
    const canvas = document.getElementById('audio-visualizer');
    if (!canvas) return;
    
    const ctx = canvas.getContext('2d');
    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    
    analyser.getByteFrequencyData(dataArray);
    
    // Clear with white background
    ctx.fillStyle = '#ffffff';
    ctx.fillRect(0, 0, canvas.width, canvas.height);
    
    const barCount = 32;
    const barWidth = (canvas.width / barCount) * 0.9;
    const barGap = (canvas.width / barCount) * 0.1;
    const borderRadius = barWidth / 2;
    
    // Calculate bar area bounds
    const totalBarsWidth = barCount * (barWidth + barGap);
    const startX = barGap / 2;
    const endX = startX + totalBarsWidth - barGap;
    
    // Draw center line
    const centerY = canvas.height / 2;
    ctx.strokeStyle = '#e0e0e0';
    ctx.lineWidth = 1;
    ctx.beginPath();
    ctx.moveTo(startX, centerY);
    ctx.lineTo(endX, centerY);
    ctx.stroke();
    
    // Focus on vocal frequency range (80Hz - 8000Hz)
    const binFrequency = 22050 / bufferLength;
    const minFreq = 80;
    const maxFreq = 8000;
    const minBin = Math.floor(minFreq / binFrequency);
    const maxBin = Math.floor(maxFreq / binFrequency);
    const usefulBins = maxBin - minBin;
    
    // Draw bars
    for (let i = 0; i < barCount; i++) {
        const start = minBin + Math.floor((i * usefulBins) / barCount);
        const end = minBin + Math.floor(((i + 1) * usefulBins) / barCount);
        
        // Average frequency data
        let sum = 0;
        for (let j = start; j < end; j++) {
            sum += dataArray[j];
        }
        const average = sum / (end - start);
        
        const fullBarHeight = (average / 255) * canvas.height * 0.85;
        const halfBarHeight = fullBarHeight / 2;
        const x = i * (barWidth + barGap) + barGap / 2;
        
        // Draw bar up
        const yUp = centerY - halfBarHeight;
        drawRoundedBar(ctx, x, yUp, barWidth, halfBarHeight, borderRadius, true);
        
        // Draw bar down
        const yDown = centerY;
        drawRoundedBar(ctx, x, yDown, barWidth, halfBarHeight, borderRadius, false);
    }
    
    window.requestAnimationFrame(visualizeAudio);
}

/**
 * Draw a rounded bar with gradient
 */
function drawRoundedBar(ctx, x, y, width, height, radius, isUp) {
    if (height < 2) return;
    
    const gradient = ctx.createLinearGradient(0, isUp ? y : y, 0, isUp ? y + height : y + height);
    gradient.addColorStop(0, '#009f95');
    gradient.addColorStop(1, '#4169B8');
    
    ctx.fillStyle = gradient;
    ctx.beginPath();
    
    if (isUp) {
        ctx.moveTo(x, y + height);
        ctx.lineTo(x, y + radius);
        ctx.arcTo(x, y, x + width, y, radius);
        ctx.lineTo(x + width, y + radius);
        ctx.arcTo(x + width, y, x + width, y + height, radius);
        ctx.lineTo(x + width, y + height);
    } else {
        ctx.moveTo(x, y);
        ctx.lineTo(x, y + height - radius);
        ctx.arcTo(x, y + height, x + width, y + height, radius);
        ctx.lineTo(x + width, y + height - radius);
        ctx.arcTo(x + width, y + height, x + width, y, radius);
        ctx.lineTo(x + width, y);
    }
    
    ctx.closePath();
    ctx.fill();
}

/**
 * Image prediction loop
 */
async function loopImage() {
    if (!isRunning || !webcam) return;
    
    webcam.update();
    await predictImage();
    window.requestAnimationFrame(loopImage);
}

/**
 * Pose prediction loop
 */
async function loopPose() {
    if (!isRunning || !webcam) return;
    
    webcam.update();
    await predictPose();
    window.requestAnimationFrame(loopPose);
}

/**
 * Predict for image model
 */
async function predictImage() {
    const model = getModel();
    if (!model || !webcam) return;
    
    const prediction = await model.predict(webcam.canvas);
    displayPredictions(prediction);
}

/**
 * Predict for pose model and draw frame + skeleton on the display canvas
 */
async function predictPose() {
    const model = getModel();
    if (!model || !webcam || !predictionCanvas) return;

    try {
        // Scale the 400x400 webcam frame down to 200x200 before estimation.
        // The TM pose classifier expects keypoint coordinates in the 0-200 range
        // (the resolution used during training).
        const predCtx = predictionCanvas.getContext('2d');
        predCtx.drawImage(webcam.canvas, 0, 0, 200, 200);

        const { pose, posenetOutput } = await model.estimatePose(predictionCanvas);
        const prediction = await model.predict(posenetOutput);

        // Draw the webcam frame and skeleton on the display canvas.
        // Using a separate canvas avoids any transform side-effects left by
        // webcam.update() on webcam.canvas's context.
        const displayCanvas = document.getElementById('pose-display');
        if (displayCanvas) {
            const ctx = displayCanvas.getContext('2d');
            ctx.drawImage(webcam.canvas, 0, 0, 400, 400);
            if (pose && pose.keypoints) {
                // Keypoints are in 0-200 space; scale by 2 for the 400x400 display
                drawPose(pose, ctx, 2);
            }
        }

        displayPredictions(prediction);
    } catch (error) {
        console.error('Pose prediction error:', error);
    }
}

/**
 * Draw pose skeleton (keypoints and connections)
 */
function drawPose(pose, ctx, scale = 1) {
    if (!pose.keypoints) return;
    
    // Draw keypoints
    pose.keypoints.forEach(keypoint => {
        if (keypoint.score > 0.5) {
            ctx.beginPath();
            ctx.arc(keypoint.position.x * scale, keypoint.position.y * scale, 5, 0, 2 * Math.PI);
            ctx.fillStyle = '#00ff00';
            ctx.fill();
        }
    });
    
    // Draw skeleton connections
    const skeleton = [
        [5, 6], [5, 7], [7, 9], [6, 8], [8, 10],
        [5, 11], [6, 12], [11, 12],
        [11, 13], [13, 15], [12, 14], [14, 16]
    ];
    
    skeleton.forEach(([i, j]) => {
        const kp1 = pose.keypoints[i];
        const kp2 = pose.keypoints[j];
        
        if (kp1.score > 0.5 && kp2.score > 0.5) {
            ctx.beginPath();
            ctx.moveTo(kp1.position.x * scale, kp1.position.y * scale);
            ctx.lineTo(kp2.position.x * scale, kp2.position.y * scale);
            ctx.strokeStyle = '#00ff00';
            ctx.lineWidth = 2;
            ctx.stroke();
        }
    });
}

/**
 * Display predictions in UI and send to micro:bit
 */
function displayPredictions(predictions) {
    const container = document.getElementById('predictions');
    if (!container) return;
    
    if (!predictions || !Array.isArray(predictions)) {
        console.error('Invalid predictions:', predictions);
        return;
    }
    
    // Filter valid predictions
    const validPredictions = predictions.filter(pred => {
        return pred.className && 
               typeof pred.probability === 'number' && 
               !isNaN(pred.probability) &&
               isFinite(pred.probability);
    });
    
    if (validPredictions.length === 0) {
        console.warn('No valid predictions');
        return;
    }
    
    // Sort by confidence
    const sorted = validPredictions.slice().sort((a, b) => b.probability - a.probability);
    
    // Display in UI
    container.innerHTML = sorted.map((pred, index) => {
        const percentage = (pred.probability * 100).toFixed(1);
        const isTop = index === 0;
        
        return `
            <div class="prediction-item ${isTop ? 'top' : ''}">
                <div class="prediction-header">
                    <span class="class-name">${pred.className}</span>
                    <span class="confidence">${percentage}%</span>
                </div>
                <div class="confidence-bar">
                    <div class="confidence-fill" style="width: ${percentage}%"></div>
                </div>
            </div>
        `;
    }).join('');
    
    // Send to micro:bit (skip background noise)
    if (isConnected() && sorted.length > 0) {
        const top = sorted[0];
        
        if (top.className.toLowerCase() !== 'ruido de fondo') {
            sendToMicrobit(top.className, top.probability * 100);
        }
    }
}

/**
 * Handle page visibility changes
 * Resume prediction loops when page becomes visible
 */
document.addEventListener('visibilitychange', function() {
    if (document.hidden) {
        console.warn('‚è∏Ô∏è Page hidden - predictions paused');
    } else {
        console.log('‚ñ∂Ô∏è Page visible - predictions resumed');
        if (isRunning) {
            const modelType = getModelType();
            if (modelType === 'image' && webcam) {
                window.requestAnimationFrame(loopImage);
            } else if (modelType === 'pose' && webcam) {
                window.requestAnimationFrame(loopPose);
            } else if (modelType === 'audio' && analyser) {
                window.requestAnimationFrame(visualizeAudio);
            }
        }
    }
});

/**
 * Toggle between front and back camera for image/pose models.
 * Returns false if the back camera is not available.
 * No upfront probe ‚Äî just tries the switch and reverts on failure.
 */
async function flipCamera() {
    const modelType = getModelType();
    if (modelType !== 'image' && modelType !== 'pose') return false;

    const prevFacingMode = currentFacingMode;
    currentFacingMode = prevFacingMode === 'user' ? 'environment' : 'user';

    // Tear down current webcam
    if (webcam) {
        webcam.stop();
        webcam = null;
    }
    predictionCanvas = null;
    document.getElementById('webcam-wrapper').innerHTML = '';

    try {
        if (modelType === 'image') {
            await setupWebcam();
        } else {
            await setupPoseWebcam();
        }
        return true;
    } catch {
        // Back camera unavailable ‚Äî revert to previous camera
        currentFacingMode = prevFacingMode;
        if (modelType === 'image') {
            await setupWebcam();
        } else {
            await setupPoseWebcam();
        }
        return false;
    }
}

export { startPredictions, stopPredictions, flipCamera };